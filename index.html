<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>APES: Articulated Part Extraction from Sprite Sheets</title>

<link media="all" href="./style.css" type="text/css" rel="stylesheet">
<style type="text/css" media="all">

pre{    
  display: block;
    padding: 4px;
    margin: 0 0 1px;
    font-size: 13px;
    line-height: 1.42857143;
    color: #333;
    word-break: break-all;
    word-wrap: break-word;
    background-color: #f5f5f5;
    border: 1px solid #ccc;
    border-radius: 3px;
}

img {
	PADDING-RIGHT: 0px;
	PADDING-LEFT: 10px;
	FLOAT: right;
	PADDING-BOTTOM: 10px;
	PADDING-TOP: 10px
}
#content {
	MARGIN-LEFT: auto;
 WIDTH: expression(document.body.clientWidth > 925? "925px": "auto" );
	MARGIN-RIGHT: auto;
	TEXT-ALIGN: left;
	max-width: 925px
}
body {
	TEXT-ALIGN: center
}
.harlow {
    font-family: "Times New Roman";
    font-size: 16px;
}
</style>
</head>
<body>
<div id="content">
  <h1 align="center">APES: Articulated Part Extraction from Sprite Sheets</h1>
  <div align="center">
  <ul id="people" align="center">
    <a href="https://people.cs.umass.edu/~zhanxu/">Zhan Xu</a> <a>, </a>
    <a href="https://techmatt.github.io/">Matthew Fisher</a> <a>, </a>
    <a href="https://people.umass.edu/~yangzhou/">Yang Zhou</a> <a>, </a>
    <a href="https://research.adobe.com/person/deepali-aneja/">Deepali Aneja</a> <a>, </a>
    <a href="https://www.linkedin.com/in/rushikesh-dudhat-274245147">Rushikesh Dudhat</a> <a>, </a>
    <a href="https://ericyi.github.io//">Li Yi</a> <a>, </a>
    <a href="http://people.cs.umass.edu/~kalo/">Evangelos Kalogerakis</a>
  </ul>
</div>

  <p align="center"><i>CVPR 2022</i></p>

  <p><img src="teaser_apes.png" width="600" align="center" style="padding: 15px 150px 15px 150px"></p>

  <h2>Abstract</h2>
  <p align="justify">Rigged puppets are one of the most prevalent representations to create 2D character animations. Creating these puppets requires partitioning characters into independently moving parts. In this work, we present a method to automatically identify such articulated parts from a small set of character poses shown in a sprite sheet, which is an illustration of the character that artists often draw before puppet creation. Our method is trained to infer articulated body parts, e.g. head, torso and limbs, that can be re-assembled to best reconstruct the given poses. Our results demonstrate significantly better performance than alternatives qualitatively and quantitatively. </p>
  
  <h2>Paper</h2>
  <p><a href="https://arxiv.org/abs/2206.02015"><img src="paper_thumb.png" alt="" width="200" height="260" style="float:left"></a></p>
  <br style="clear:both" />

  <h2>Poster</h2>
  <p><a href="https://people.cs.umass.edu/~zhanxu/projects/APES/POSTER_APES.pdf"><img src="poster_thumb.png" alt="" width="600" height="300" style="float:left"></a></p>
    
  <br style="clear:both" />
  <h2>Video</h2>
  <iframe width="640" height="360" src="https://www.youtube.com/embed/YQtbRXFKNZE" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  
  <br>
  <h2>CVPR talk</h2>
 <iframe width="640" height="360" src="https://www.youtube.com/embed/XCf68SKsaRk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
   
  <!-- <h2>30s fast-forward demo for SIGGRAPH 2020</h2>
 <iframe width="640" height="360" src="https://www.youtube.com/embed/LL2KdYxr26c?list=PL1U-zmoZcBllPJ8yG4vDGc_Cjvshero22" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
  
  <br> 
  </p><h2>Source Code &amp; Data</h2>
  Coming soon!
  <!-- <p>Github code: <a href="https://github.com/zhan-xu/RigNet">https://github.com/zhan-xu/RigNet</a></p>

  <p>Dataset: <a href="https://umass.box.com/s/448zm5iw1ewbq4l2kdll6q99v5y3q4pw"><b class="harlow">ModelsResource-RigNetv1</b> </a> </p>
	
  Our code is licensed under the General Public License Version 3 (GPLv3), or under a Commercial License. To inquire into a Commercial License, please contact the University of Massachusetts Amherst Technology Transfer Office at tto@umass.edu, and copy to kalo@cs.umass.edu and zhanxu@cs.umass.edu. -->


  <!-- <h2>Citation</h2>
  If you use our dataset or code, please cite the following papers.
  <pre>
  @InProceedings{AnimSkelVolNet,
    title={Predicting Animation Skeletons for 3D Articulated Models via Volumetric Nets},
    author={Zhan Xu and Yang Zhou and Evangelos Kalogerakis and Karan Singh},
    booktitle={2019 International Conference on 3D Vision (3DV)},
    year={2019}
  }</pre>
  <pre>
  @article{RigNet,
    title={RigNet: Neural Rigging for Articulated Characters},
    author={Zhan Xu and Yang Zhou and Evangelos Kalogerakis and Chris Landreth and Karan Singh},
    journal={ACM Trans. on Graphics},
    year={2020},
    volume={39}
  }</pre>  

  <h2> Acknowledgements </h2>
  <p>This research is partially funded by NSF (EAGER-1942069) and NSERC. Our experiments were performed in the UMass GPU cluster obtained under the Collaborative Fund managed by the Massachusetts Technology Collaborative.</p> -->
</div>


</body></html
